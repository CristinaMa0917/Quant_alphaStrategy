
# coding: utf-8

# ### 3.1ã€æ•°æ®å‡†å¤‡ </font>
# ---
# - è¯¥éƒ¨åˆ†è€—æ—¶ **10åˆ†é’Ÿå·¦å³**
# 
# ---
# è¯¥éƒ¨åˆ†å†…å®¹ä¸ºï¼š
# - ä»uqerçš„DataAPIä¸­è·å–70ä¸ªå› å­çš„æ•°å€¼ï¼Œå–å¾—å› å­å¯¹åº”çš„ä¸‹ä¸€æœŸçš„è‚¡ä»·æ¶¨è·Œæ•°æ®ï¼Œå¹¶å°†æ•°æ®è¿›è¡Œå¯¹é½
# 
# - å¯¹å› å­è¿›è¡Œç¼ºå¤±å€¼å¡«å……ã€winsorize,neutralize,standardizeå¤„ç†
# 
# - å¯¹ä¸Šè¿°æ•°æ®è¿›è¡Œå¤„ç†ï¼Œæ–¹ä¾¿åç»­åˆ†ç±»æ¨¡å‹è¿›è¡Œè®­ç»ƒã€æµ‹è¯•

# ##### 3.1.1 è·å–å› å­æ•°æ®å’Œè‚¡ä»·æ¶¨è·Œæ•°æ®

# - è·å–å› å­çš„åŸå§‹æ•°æ®å€¼, å¹¶å°†æ‰€éœ€è¦çš„æ•°æ®éƒ½å­˜ä¸‹æ¥ï¼Œä¾¿äºåé¢çš„æ¨¡å‹å‚æ•°è°ƒæ•´ã€ä¼˜åŒ–ï¼ŒèŠ‚çœæ—¶é—´ã€‚ç”Ÿæˆraw_data/factor_chpct.csv 
# 
# - æ•°æ®æ–‡ä»¶çš„æ ¼å¼ä¸ºï¼š
# ![factor_chpct.csvc](http://odqb0lggi.bkt.clouddn.com/560a3e12f9f06c597165ef9c/01bf8e8a-2026-11e8-927b-0242ac140002)
# 
# 
# - è¯¥æ®µä»£ç ç”¨äº†å¤šçº¿ç¨‹åŠ é€Ÿ(ä»£ç 62è¡Œï¼šThreadPool(processes=16)ï¼‰ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·è‡ªå·±è¿è¡Œç¯å¢ƒè¿›è¡Œè°ƒæ•´çº¿ç¨‹æ•°ã€‚

# In[ ]:

universe = set_universe('HS300')+set_universe('ZZ500')
end = '20180517'


# In[ ]:



import pandas as pd
import numpy as np
import os
import time
import multiprocessing
from multiprocessing import Pool
from multiprocessing.dummy import Pool as ThreadPool

raw_data_dir = "./raw_data"
if not os.path.exists(raw_data_dir):
    os.mkdir(raw_data_dir)

#å®šä¹‰70ä¸ªå› å­
factors = [b'Beta60', b'OperatingRevenueGrowRate', b'NetProfitGrowRate', b'NetCashFlowGrowRate', b'NetProfitGrowRate5Y',
           b'TVSTD20',
           b'TVSTD6', b'TVMA20', b'TVMA6', b'BLEV', b'MLEV', b'CashToCurrentLiability', b'CurrentRatio', b'REC',
           b'DAREC', b'GREC',
           b'DASREV', b'SFY12P', b'LCAP', b'ASSI', b'LFLO', b'TA2EV', b'PEG5Y', b'PE', b'PB', b'PS', b'SalesCostRatio',
           b'PCF', b'CETOP',
           b'TotalProfitGrowRate', b'CTOP', b'MACD', b'DEA', b'DIFF', b'RSI', b'PSY', b'BIAS10', b'ROE', b'ROA',
           b'ROA5', b'ROE5',
           b'DEGM', b'GrossIncomeRatio', b'ROECut', b'NIAPCut', b'CurrentAssetsTRate', b'FixedAssetsTRate', b'FCFF',
           b'FCFE', b'PLRC6',
           b'REVS5', b'REVS10', b'REVS20', b'REVS60', b'HSIGMA', b'HsigmaCNE5', b'ChaikinOscillator',
           b'ChaikinVolatility', b'Aroon',
           b'DDI', b'MTM', b'MTMMA', b'VOL10', b'VOL20', b'VOL5', b'VOL60', b'RealizedVolatility', b'DASTD', b'DDNSR',
           b'Hurst']

def get_factor_by_day(tdate):
    '''
    è·å–ç»™å®šæ—¥æœŸçš„å› å­ä¿¡æ¯
    å‚æ•°ï¼š 
        tdate, æ—¶é—´ï¼Œæ ¼å¼%Y%m%d
    è¿”å›:
        DataFrame, è¿”å›ç»™å®šæ—¥æœŸçš„70ä¸ªå› å­å€¼
    '''
    cnt = 0
    while True:
        try:
            x = DataAPI.MktStockFactorsOneDayProGet(tradeDate=tdate,secID=universe,ticker=u"",field=['ticker', 'tradeDate'] + factors,pandas="1")
            x['tradeDate'] = x['tradeDate'].apply(lambda x: x.replace("-", ""))

            return x
        except Exception as e:
            cnt += 1
            if cnt >= 3:
                print('error get factor data: ', tdate)
                break


if __name__ == "__main__":
    start_time = time.time()

    # æ‹¿åˆ°äº¤æ˜“æ—¥å†ï¼Œå¾—åˆ°æœˆæœ«æ—¥æœŸ
    trade_date = DataAPI.TradeCalGet(exchangeCD=u"XSHG", beginDate="20070101", endDate=end, field=u"", pandas="1")
    trade_date = trade_date[trade_date.isMonthEnd == 1]

    print("begin to get factor value for each stock...")
    # # å–å¾—æ¯ä¸ªæœˆæœ«æ—¥æœŸï¼Œæ‰€æœ‰è‚¡ç¥¨çš„å› å­å€¼
    pool = ThreadPool(processes=16)
    date_list = [tdate.replace("-", "") for tdate in trade_date.calendarDate.values if tdate < "20180430"]
    frame_list = pool.map(get_factor_by_day, date_list)
    pool.close()
    pool.join()
    print ("ALL FINISHED")

    factor_csv = pd.concat(frame_list, axis=0)
    factor_csv.reset_index(inplace=True, drop=True)
    stock_list = np.unique(factor_csv.ticker.values)

    ########################## å–å¾—ä¸ªè‚¡å’ŒæŒ‡æ•°çš„è¡Œæƒ…æ•°æ® ################################
    print("\nbegin to get price ratio for stocks and index ...")
    # ä¸ªè‚¡ç»å¯¹æ¶¨å¹…
    chgframe = DataAPI.MktEqumAdjGet(secID=u"", ticker=stock_list, monthEndDate=u"", isOpen=u"", beginDate=u"20070131",
                                      endDate=end, field=['ticker', 'endDate', 'tradeDays', 'chgPct', 'return'], pandas="1")
    
    chgframe['endDate'] = chgframe['endDate'].apply(lambda x: x.replace("-", ""))

    # æ²ªæ·±300æŒ‡æ•°æ¶¨å¹…
    hs300_chg_frame = DataAPI.MktIdxmGet(beginDate=u"20070131", endDate=end, indexID=u"000300.ZICN", ticker=u"",
                                         field=['ticker', 'endDate', 'chgPct'], pandas="1")
    hs300_chg_frame['endDate'] = hs300_chg_frame['endDate'].apply(lambda x: x.replace("-", ""))
    hs300_chg_frame.head()

    # å¾—åˆ°ä¸ªè‚¡çš„ç›¸å¯¹æ”¶ç›Š
    hs300_chg_frame.columns = ['HS300', 'endDate', 'HS300_chgPct']
    pframe = chgframe.merge(hs300_chg_frame, on=['endDate'], how='left')
    pframe['active_return'] = pframe['chgPct'] - pframe['HS300_chgPct']
    pframe = pframe[['ticker', 'endDate', 'return', 'active_return']]
    pframe.rename(columns={"return": "abs_return"}, inplace=True)

    ################################ å¯¹é½æ•°æ® ################################
    print("begin to align data ...")
    # å¾—åˆ°æœˆåº¦å…³ç³»
    month_frame = trade_date[['calendarDate', 'isOpen']]
    month_frame['prev_month_end'] = month_frame['calendarDate'].shift(1)
    month_frame = month_frame[['prev_month_end', 'calendarDate']]
    month_frame.columns = ['month_end', 'next_month_end']
    month_frame.dropna(inplace=True)
    month_frame['month_end'] = month_frame['month_end'].apply(lambda x: x.replace("-", ""))
    month_frame['next_month_end'] = month_frame['next_month_end'].apply(lambda x: x.replace("-", ""))

    # å¯¹é½æœˆåº¦å…³ç³»
    factor_frame = factor_csv.merge(month_frame, left_on=['tradeDate'], right_on=['month_end'], how='left')

    # å¾—åˆ°ä¸ªè‚¡ä¸‹ä¸ªæœˆçš„æ¶¨å¹…æ•°æ®
    factor_frame = factor_frame.merge(pframe, left_on=['ticker', 'next_month_end'], right_on=['ticker', 'endDate'])

    del factor_frame['month_end']
    del factor_frame['endDate']

    ################################ æ•°æ®å­˜å‚¨ä¸‹æ¥ ################################
    factor_frame.to_csv(os.path.join(raw_data_dir, 'factor_chpct_2.csv'), chunksize=1000)

    end_time = time.time()
    print ("Time cost: %s seconds" % (end_time - start_time))


# In[ ]:

df = pd.read_csv(os.path.join(raw_data_dir, 'factor_chpct_2.csv'))
df.tail()


# #### 3.1.2 å¯¹æ•°æ®è¿›è¡Œwinsorize, neutralize, standardize ï¼ˆ6åˆ†é’Ÿï¼‰

# - winsorize
# 	- ä¸Šç•Œå€¼=å› å­å‡å€¼+5*|å¹³å‡å€¼ï¼ˆå› å­å€¼-å› å­å‡å€¼ï¼‰|ï¼Œä¸‹ç•Œå€¼=å› å­å‡å€¼-5*|å¹³å‡å€¼ï¼ˆå› å­å€¼-å› å­å‡å€¼ï¼‰|ï¼Œè¶…è¿‡ä¸Šä¸‹ç•Œçš„å€¼ç”¨ä¸Šä¸‹ç•Œå€¼å¡«å……
# 
# 
# - å¯¹æ•°æ®ç©ºå€¼è¿›è¡Œå¡«å……ï¼š ç”¨åŒæœŸç”³ä¸‡ä¸€çº§**è¡Œä¸šçš„å‡å€¼**è¿›è¡Œç©ºå€¼å¡«å……
# 
# - neutralizeå’Œstandardize
# 	- ç›´æ¥è°ƒç”¨ä¼˜çŸ¿çš„neutralizeå‡½æ•°è¿›è¡Œä¸­æ€§åŒ–ï¼Œä¸­æ€§æ—¶å€™ä¸åŒ…æ‹¬'BETA', 'RESVOL', 'MOMENTUM', 'EARNYILD', 'BTOP', 'GROWTH', 'LEVERAGE', 'LIQUIDTY'ä»¥å’Œç ”æŠ¥ä¸€è‡´
# 	
# 	- å¯¹ä¸­æ€§åŒ–åçš„å› å­è¿›è¡Œæ ‡å‡†åŒ–ï¼Œç›´æ¥è°ƒç”¨ä¼˜çŸ¿çš„standardizeå‡½æ•°
#  
# - å¤„ç†åçš„æ–‡ä»¶å­˜å‚¨åœ¨ raw_data/after_prehandle.csv, æ–‡ä»¶çš„æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
# ![å›¾ç‰‡æ³¨é‡Š](http://odqb0lggi.bkt.clouddn.com/560a3e12f9f06c597165ef9c/b9d5730e-20ee-11e8-85a4-0242ac140002)

# In[ ]:


import pandas as pd
import numpy as np
import os
import shutil
import multiprocessing
import time
import gevent
from multiprocessing import Pool
from multiprocessing.dummy import Pool as ThreadPool


######################################### é€šç”¨å˜é‡è®¾ç½® #########################################
start_time = time.time()
raw_data_dir = "./raw_data"

pre_handle_dir = "./pre_handle_data"  # å­˜æ”¾ä¸­é—´æ•°æ®
if not os.path.exists(pre_handle_dir):
    os.mkdir(pre_handle_dir)

# ç”³ä¸‡ä¸€çº§è¡Œä¸šåˆ†ç±»
sw_map_frame = DataAPI.EquIndustryGet(industryVersionCD=u"010303", industry=u"", secID=u"", ticker=u"", intoDate=u"",field=[u'ticker', 'secShortName', 'industry', 'intoDate', 'outDate', 'industryName1', 'industryName2', 'industryName3', 'isNew'], pandas="1")
sw_map_frame = sw_map_frame[sw_map_frame.isNew == 1]
    

# è¯»å…¥åŸå§‹å› å­
input_frame = pd.read_csv(os.path.join(raw_data_dir, u'factor_chpct_2.csv'),
                          dtype={"ticker": np.str, "tradeDate": np.str, "next_month_end": np.str}, index_col=0)

# å¾—åˆ°å› å­å
extra_list = ['ticker', 'tradeDate', 'next_month_end', 'abs_return', 'active_return']
factor_name = [x for x in input_frame.columns if x not in extra_list]

print('init data done, cost time: %s seconds' % (time.time()-start_time))

################################### å®šä¹‰æ•°æ®å¤„ç†çš„ä¸€äº›åŸºæœ¬å‡½æ•° ##################################

def paper_winsorize(v, upper, lower):
    '''
    winsorizeå»æå€¼ï¼Œç»™å®šä¸Šä¸‹ç•Œ
    å‚æ•°:    
        v: Series, å› å­å€¼
        upper: ä¸Šç•Œå€¼
        lower: ä¸‹ç•Œå€¼
    è¿”å›:
        Series, è§„å®šä¸Šä¸‹ç•Œåå› å­å€¼
    '''
    if v > upper:
        v = upper
    elif v < lower:
        v = lower
    return v

def winsorize_by_date(cdate_input):
    '''
    æŒ‰ç…§[dm+5*dm1, dm-5*dm1]è¿›è¡Œwinsorize
    å‚æ•°:
        cdate_input: æŸä¸€æœŸçš„å› å­å€¼çš„dataframe
    è¿”å›:
        DataFrame, å»æå€¼åçš„å› å­å€¼
    '''
    media_v = cdate_input.median()
    for a_factor in factor_name:
        dm = media_v[a_factor]
        new_factor_series = abs(cdate_input[a_factor] - dm)  # abs(di-dm)
        dm1 = new_factor_series.median()
        upper = dm + 5 * dm1
        lower = dm - 5 * dm1
        cdate_input[a_factor] = cdate_input[a_factor].apply(lambda x: paper_winsorize(x, upper, lower))
    return cdate_input


def nafill_by_sw1(cdate_input):
    '''
    ç”¨ç”³ä¸‡ä¸€çº§çš„å‡å€¼è¿›è¡Œå¡«å……
    å‚æ•°:
        cdate_input: å› å­å€¼ï¼ŒDataFrame
    è¿”å›:
        DataFrame, å¡«å……ç¼ºå¤±å€¼åçš„å› å­å€¼
    '''
    func_input = cdate_input.copy()
    func_input = func_input.merge(sw_map_frame[['ticker', 'industryName1']], on=['ticker'], how='left')
    
    func_input.loc[:, factor_name] = func_input.loc[:, factor_name].fillna(func_input.groupby('industryName1')[factor_name].transform("mean"))
    
    return func_input.fillna(0.0)


def winsorize_fillna_date(tdate):
    '''
    å¯¹æŸä¸€å¤©çš„æ•°æ®è¿›è¡Œå»æå€¼ï¼Œå¡«å……ç¼ºå¤±å€¼
    å‚æ•°:
        tdateï¼š æ—¶é—´ï¼Œ æ ¼å¼ä¸º %Y%m%d
    è¿”å›:
        DataFrame, å»æå€¼ï¼Œå¡«å……ç¼ºå¤±å€¼åçš„å› å­å€¼
    '''
    cnt = 0
    while True:
        try:
            cdate_input = input_frame[input_frame.tradeDate == tdate]
            # print("####Running single_date for %s" % tdate)
            # winsorize
            cdate_input = winsorize_by_date(cdate_input)

            # ç¼ºå¤±å€¼å¡«å……, ç”¨åŒè¡Œä¸šçš„å‡å€¼
            cdate_input = nafill_by_sw1(cdate_input)
            cdate_input.set_index('ticker', inplace=True)

            return cdate_input
        except Exception as e:
            cnt += 1
            if cnt >= 3:
                cdate_input = input_frame[input_frame.tradeDate == tdate]
                # ç¼ºå¤±å€¼å¡«å……, ç”¨åŒè¡Œä¸šçš„å‡å€¼
                cdate_input = nafill_by_sw1(cdate_input)
                cdate_input.set_index('ticker', inplace=True)
                return cdate_input
            
            
def standardize_neutralize_factor(input_data):
    '''
    è¡Œä¸šã€å¸‚å€¼ä¸­æ€§åŒ–ï¼Œå¹¶è¿›è¡Œæ ‡å‡†åŒ–
    å‚æ•°: 
        input_dataï¼štuple, ä¼ å…¥çš„æ˜¯(å› å­å€¼ï¼Œæ—¶é—´)ã€‚å› å­å€¼ä¸ºDataFrame
    è¿”å›:
        DataFrame, è¡Œä¸šã€å¸‚å€¼ä¸­æ€§åŒ–ï¼Œå¹¶è¿›è¡Œæ ‡å‡†åŒ–åçš„å› å­å€¼
    '''
    cdate_input, tdate = input_data
    for a_factor in factor_name:
        cnt = 0
        while True:
            try:
                cdate_input.loc[:, a_factor] = standardize(neutralize(cdate_input[a_factor], target_date=tdate,
                        exclude_style_list=['BETA', 'RESVOL', 'MOMENTUM', 'EARNYILD', 'BTOP', 'GROWTH', 'LEVERAGE', 'LIQUIDTY']))
                break
            except Exception as e:
                cnt += 1
                if cnt >= 3:
                    break
    
    return cdate_input

            
if __name__ == "__main__":
    ############################################ å¯¹æ¯æœŸçš„æ•°æ®è¿›è¡Œå¤„ç† ###########################################
    # éå†æ¯ä¸ªæœˆæœ«æ—¥æœŸï¼Œå¯¹å› å­è¿›è¡Œå»æå€¼ã€ç©ºå€¼å¡«å……
    print('winsorize factor data...')
    pool = Pool(processes=8)
    date_list = [tdate for tdate in np.unique(input_frame.tradeDate.values) if int(tdate) > 20061231]
    dframe_list = pool.map(winsorize_fillna_date, date_list)

    # éå†æ¯ä¸ªæœˆæœ«æ—¥æœŸï¼Œåˆ©ç”¨åç¨‹å¯¹å› å­è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä¸­æ€§åŒ–å¤„ç†
    print('standardize & neutralize factor...')
    jobs = [gevent.spawn(standardize_neutralize_factor, value) for value in zip(dframe_list, date_list)]
    gevent.joinall(jobs)
    new_dframe_list = [e.value for e in jobs]
    print('standardize neutralize factor finished!')
    
            
    # å°†ä¸åŒæœˆä»½çš„æ•°æ®åˆå¹¶åˆ°ä¸€èµ·
    all_frame = pd.concat(new_dframe_list, axis=0)
    all_frame.reset_index(inplace=True)

    # å­˜å‚¨ä¸‹æ¥
    all_frame.to_csv(os.path.join(raw_data_dir, "after_prehandle_2.csv"), encoding='gbk', chunksize=1000)
    end_time = time.time()
    print("\nData handle finished! Time Cost:%s seconds" % (end_time - start_time))


# In[ ]:

d = pd.read_csv(os.path.join(raw_data_dir, "after_prehandle_2.csv"))
d.tail()


# #### 3.1.3 æ¨¡å‹æ•°æ®å‡†å¤‡(2åˆ†é’Ÿ)

# - ç»™åŸå§‹æ•°æ®æ‰“ä¸Šæ ‡ç­¾ï¼Œåœ¨æ¯ä¸ªæœˆæœ«æˆªé¢æœŸï¼Œé€‰å–ä¸‹æœˆæ”¶ç›Šæ’åå‰30%çš„è‚¡ç¥¨ä½œä¸ºæ­£ä¾‹ï¼ˆğ‘¦=1ï¼‰ï¼Œå30%çš„è‚¡ç¥¨ä½œä¸ºè´Ÿä¾‹ï¼ˆğ‘¦=âˆ’1ï¼‰ï¼Œå…¶ä½™çš„è‚¡ç¥¨æ ‡ç­¾ä¸º0.
# 
# - å¤„ç†åçš„æ–‡ä»¶å­˜å‚¨åœ¨ raw_data/dataset.csv, æ–‡ä»¶çš„æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
# ![å›¾ç‰‡æ³¨é‡Š](http://odqb0lggi.bkt.clouddn.com/55adb48bf9f06c94497ec025/a05797c4-21aa-11e8-85a4-0242ac140002)

# In[ ]:

import pandas as pd
import numpy as np
import os
import time

start_time = time.time()
raw_data_dir = "./raw_data"

def get_label_by_return(filename):
    '''
    å¯¹ä¸‹æœŸæ”¶ç›Šæ‰“æ ‡ç­¾ï¼Œæ¶¨å¹…top 30%ä¸º+1ï¼Œè·Œå¹…top 30%ä¸º-1
    å‚æ•°:
        filenameï¼šcsvæ–‡ä»¶åï¼Œä¸ºä¸Šè¯‰æ­¥éª¤ä¿å­˜çš„å› å­å€¼
    è¿”å›:
        DataFrame, æ‰“å®Œæ ‡ç­¾åçš„æ•°æ®
    '''
    df = pd.read_csv(filename, dtype={"ticker": np.str, "tradeDate": np.str, "next_month_end": np.str},index_col=0, encoding='gb2312').fillna(0.0)

    new_df = None
    for date, group in df.groupby('tradeDate'):
        quantile_30 = group['active_return'].quantile(0.3)
        quantile_70 = group['active_return'].quantile(0.7)

        def _get_label(x):
            if x >= quantile_70:
                return 1
            elif x <= quantile_30:
                return -1
            else:
                return 0

        group.loc[:, 'label'] = group.loc[:, 'active_return'].apply(lambda x : _get_label(x))

        if new_df is None:
            new_df = group
        else:
            new_df = pd.concat([new_df, group],ignore_index=True)

    return new_df
new_df = get_label_by_return(os.path.join(raw_data_dir, "after_prehandle_2.csv"))
new_df['year'] = new_df['next_month_end'].apply(lambda x: int(int(x)/10000))
new_df.to_csv(os.path.join(raw_data_dir, "dataset_2.csv"), encoding='gbk', chucksize=1000)

print("Done, Time Cost:%s seconds" % (time.time() - start_time))


# In[ ]:

new_df.tail()


# ### 3.2ã€æ¨¡å‹å› å­åˆæˆ </font>
# ---
# - è¯¥éƒ¨åˆ†è€—æ—¶ **2åˆ†é’Ÿå·¦å³**
# 
# ---
# 
# è¯¥éƒ¨åˆ†åˆ†ä¸º2ä¸ªé˜¶æ®µ
# -  ç¬¬ä¸€ä¸ªé˜¶æ®µè®­ç»ƒæ¨¡å‹, å¹¶åˆæˆå› å­
# -  ç¬¬äºŒä¸ªé˜¶æ®µåˆ†ææ¨¡å‹çš„å‡†ç¡®åº¦

# #### 3.2.1 æ¨¡å‹è®­ç»ƒ(1åˆ†é’Ÿå·¦å³)

# - ä¸ºäº†èƒ½è®©æ¨¡å‹åŠæ—¶æŠ“å–åˆ°å¸‚åœºçš„å˜åŒ–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸ƒä¸ªé˜¶æ®µæ»šåŠ¨å›æµ‹æ–¹æ³•ã€‚æ¨¡å‹è®­ç»ƒåŒºé—´ä¸º20070101è‡³20171231ï¼ŒæŒ‰å¹´ä»½åˆ†ä¸º7ä¸ªå­åŒºé—´
# - æ•°æ®åˆ’åˆ†å’Œå›æµ‹è®¾ç½®çš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼ˆç”±äºå› å­æ•°æ®ä»2007å¹´å¼€å§‹ï¼Œå› æ­¤å‰é¢å‡ å¹´çš„è®­ç»ƒæ•°æ®ç¨çŸ­ä¸€äº›ï¼‰ï¼š
# ![å›¾ç‰‡æ³¨é‡Š](http://odqb0lggi.bkt.clouddn.com/560a3e12f9f06c597165ef9c/9579776c-2071-11e8-927b-0242ac140002)
# 
# - æ ¹æ®åæ³°ç ”æŠ¥ä¸­çš„æ€è·¯ï¼Œåœ¨è®­ç»ƒä¸­ä¼šä¸¢å¼ƒæ‰æ¶¨è·Œå¹…å¤„äºä¸­é—´ä½ç½®(label=0)çš„æ ·æœ¬ï¼Œä»¥å‡å°‘éšæœºå™ªå£°çš„å½±å“

# In[ ]:

#--------------------------------åŠ è½½ç¬¬ä¸€éƒ¨åˆ†çš„é¢„å¤„ç†æ•°æ®-----------------------------------------------
import os
import pandas as pd
import numpy as np


factors = [b'Beta60', b'OperatingRevenueGrowRate', b'NetProfitGrowRate', b'NetCashFlowGrowRate', b'NetProfitGrowRate5Y', b'TVSTD20',
           b'TVSTD6', b'TVMA20', b'TVMA6', b'BLEV', b'MLEV', b'CashToCurrentLiability', b'CurrentRatio', b'REC', b'DAREC', b'GREC',
           b'DASREV', b'SFY12P', b'LCAP', b'ASSI', b'LFLO', b'TA2EV', b'PEG5Y', b'PE', b'PB', b'PS', b'SalesCostRatio', b'PCF', b'CETOP',
           b'TotalProfitGrowRate', b'CTOP', b'MACD', b'DEA', b'DIFF', b'RSI', b'PSY', b'BIAS10', b'ROE', b'ROA', b'ROA5', b'ROE5',
           b'DEGM', b'GrossIncomeRatio', b'ROECut', b'NIAPCut', b'CurrentAssetsTRate', b'FixedAssetsTRate', b'FCFF', b'FCFE', b'PLRC6',
           b'REVS5', b'REVS10', b'REVS20', b'REVS60', b'HSIGMA', b'HsigmaCNE5', b'ChaikinOscillator', b'ChaikinVolatility', b'Aroon',
           b'DDI', b'MTM', b'MTMMA', b'VOL10', b'VOL20', b'VOL5', b'VOL60', b'RealizedVolatility', b'DASTD', b'DDNSR', b'Hurst']
raw_data_dir = "./raw_data"
df = pd.read_csv(os.path.join(raw_data_dir, "dataset.csv"), dtype={"ticker": np.str, "tradeDate": np.str, "next_month_end": np.str},index_col=0, encoding='GBK')
df.head()


# In[ ]:

#--------------------------------æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é˜¶æ®µ-----------------------------------------------

import time
import os

def get_train_val_test_data(df, year):
    '''
    #ç»™å®šå¹´ä»½ï¼Œæ‹†åˆ†è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†
    å‚æ•°:
        df: ç¬¬ä¸€éƒ¨åˆ†å¤„ç†åçš„æ•°æ®ï¼Œå­˜å‚¨åœ¨./raw_data/dataset.csv
        year: å¹´ä»½ï¼Œæ˜¯ä¸Šè¿°7ä¸ªæ»šåŠ¨æµ‹è¯•é˜¶æ®µçš„åŒºåˆ«å­—æ®µ
    è¿”å›:
        train_df,  test_df: å‡ä¸ºDataFrameï¼Œå¯¹åº”ç€è®­ç»ƒã€æµ‹è¯•çš„æ•°æ®é›†
    '''
    
    back_year = max(2007, year-6)
    train_df = df[(df['year']>=back_year) & (df['year']<year)]
    
    test_df = df[df['year']==year]
    
    return train_df, test_df


def format_feature_label(origin_df, is_filter=True):
    '''
    è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
    å‚æ•°:
        origin_df: åŸå§‹è¾“å…¥æ•°æ®ï¼ŒDataFrame
        is_filter: æ˜¯å¦éœ€è¦è¿‡æ»¤labelä¸º0çš„æ•°æ®
    è¿”å›:
        feature, np.array, å¯¹åº”ç€æ›´æ”¹æ ¼å¼åçš„æ•°æ®ç‰¹å¾
        label, np.array, å¯¹åº”ç€æ›´æ”¹æ ¼å¼åçš„æ•°æ®æ ‡ç­¾
    '''
    
    if is_filter:
        origin_df = origin_df[origin_df['label']!=0]
        #æ¨¡å‹çš„labelè¾“å…¥èŒƒå›´æ›¿æ¢æˆ[0, 1]ï¼Œæ¯”è¾ƒç›´è§‚ï¼Œéœ€è¦å¯¹åŸå§‹labelè¿›è¡Œæ›¿æ¢
        origin_df['label'] = origin_df['label'].replace(-1, 0)
        
    feature = np.array(origin_df[factors])
    label = np.array(origin_df['label'])

    return feature, label

def write_factor_to_csv(df, predict_score, year, filename):
    '''
    è®°å½•æ¨¡å‹é¢„æµ‹åˆ†æ•°ä¸ºå› å­å€¼ï¼Œè¾“å‡º
    å‚æ•°:
        dfï¼š åŸå§‹æ•°æ®ï¼Œ DataFrame
        predict_score: æ¨¡å‹é¢„æµ‹çš„åˆ†æ•°
        year: å¹´ä»½
        filename: éœ€è¦ä¿å­˜çš„æ–‡ä»¶åç§°
    '''
    
    df['factor'] = predict_score
    df = df.loc[:, ['ticker', 'tradeDate', 'label', 'factor']]
    is_header = True
    if year != 2011:
        is_header = False
    
    df.to_csv(filename, mode='a+', encoding='utf-8', header=is_header)


from sklearn.ensemble import RandomForestClassifier  
def get_rf_result(train_data, train_label, test_data): #model 4
    rf = RandomForestClassifier(n_estimators = 50,min_samples_split = 50,min_samples_leaf =13,                                n_jobs = 8,max_depth = 14)
    rf.fit(train_data, train_label)
    predict_score = rf.predict_proba(test_data)[:, 1]   
    return predict_score


def pipeline():
    '''
    å¯¹7ä¸ªé˜¶æ®µåˆ†åˆ«è¿›è¡Œè®­ç»ƒæµ‹è¯•ï¼Œå¹¶ä¿å­˜æµ‹è¯•çš„å› å­åˆæˆå€¼
    è¿”å›:
        boost_model_list, listç»“æ„ï¼Œæ¯ä¸ªé˜¶æ®µæ±‡æ€»çš„æ¨¡å‹é›†åˆ
    '''
    
    t0 = time.time()
    raw_data_dir = "./raw_data"
    
    linear_file = os.path.join(raw_data_dir, "factor_linear_1.csv")
    try:
        os.remove(linear_file) #åˆ é™¤å†å²å› å­æ–‡ä»¶ï¼Œä»¥é˜²å†²çª
        os.remove(pca_linear_file)
    except Exception as e:
        pass
    
    boost_model_list = []
    for year in range(2011, 2018):
        print('training model for %s' % year)
        t1 = time.time()
        #æ„å»ºè®­ç»ƒæµ‹è¯•æ•°æ®
        train_df, test_df = get_train_val_test_data(df, year)
        train_feature, train_label = format_feature_label(train_df)
        test_feature, test_label = format_feature_label(test_df, False)
        
        #çº¿æ€§é€»è¾‘å›å½’æ¨¡å‹è®­ç»ƒï¼Œå¾—åˆ°å› å­å€¼è¾“å‡º
        predict_score = get_rf_result(train_feature, train_label, test_feature)
        write_factor_to_csv(test_df, predict_score, year, linear_file)
        
        print('------------------ finish year: %s, time cost: %s seconds--------------' % (year, time.time() - t1))
    
    print('Done, Time cost: %s seconds' % (time.time() - t0))

    
pipeline()


# #### 3.2.2 æ¨¡å‹ç»“æœåˆ†æ(1åˆ†é’Ÿ)

# - è®¡ç®—çŸ¥7ä¸ªé˜¶æ®µçš„å¹³å‡å‡†ç¡®ç‡åœ¨56%å·¦å³ï¼Œå¹³å‡AUCåœ¨59%å·¦å³
# - AUCçš„å…·ä½“æ„ä¹‰å‚è§æ–‡æ¡£[AUCé‡Šä¹‰](https://www.zhihu.com/question/39840928)

# In[ ]:

from datetime import datetime
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

def get_test_auc_acc(filename):
    '''
    è®¡ç®—äºŒåˆ†ç±»æ¨¡å‹æ ·æœ¬å¤–çš„ACCä¸AUCï¼ŒæŒ‰ç…§æ—¥æœŸç»Ÿè®¡
    è¿”å›:
        acc_list: æ ·æœ¬å¤–çš„é¢„æµ‹å‡†ç¡®ç‡é›†åˆ
        auc_list: æ ·æœ¬å¤–çš„é¢„æµ‹AUCé›†åˆ
        mean_acc: æ ·æœ¬å¤–çš„å¹³å‡é¢„æµ‹å‡†ç¡®ç‡
        mean_auc: æ ·æœ¬å¤–çš„å¹³å‡é¢„æµ‹AUC
    '''
    
    df = pd.read_csv(filename)
    #åªæŸ¥çœ‹åŸæœ‰labelä¸º+1, -1çš„æ•°æ®
    df = df[df['label'] != 0]
    df.loc[:, 'predict'] = df.loc[:, 'factor'].apply(lambda x : 1 if x > 0.5 else -1)

    acc_list = []  #ä¿å­˜æ¯ä¸ªæœˆä»½çš„å‡†ç¡®ç‡
    auc_list = []  #ä¿å­˜æ¯ä¸ªæœˆä»½çš„AUCæŒ‡æ ‡
    for date, group in df.groupby('tradeDate'):
        df_correct = group[group['predict'] == group['label']]
        correct = len(df_correct) * 1.0 / len(group)
        auc =  roc_auc_score(np.array(group['label']), np.array(group['factor']))
        acc_list.append([date, correct])
        auc_list.append([date, auc])
        
    acc_list = sorted(acc_list, key=lambda x: x[0], reverse=False)
    mean_acc = sum([item[1] for item in acc_list]) / len(acc_list)
    
    auc_list = sorted(auc_list, key=lambda x: x[0], reverse=False)
    mean_auc = sum([item[1] for item in auc_list]) / len(auc_list)
    
    return acc_list, auc_list, round(mean_acc, 2), round(mean_auc, 2)

def plot_accuracy_curve(filename):
    '''
    ç”»å›¾
    '''
    acc_list, auc_list, mean_acc, mean_auc = get_test_auc_acc(filename)

    plt.plot([datetime.strptime(str(item[0]), '%Y%m%d') for item in acc_list], [item[1] for item in acc_list], '-bo')
    plt.plot([datetime.strptime(str(item[0]), '%Y%m%d') for item in auc_list], [item[1] for item in auc_list], '-ro')

    plt.legend([u"acc curve: mean_acc:%s"%mean_acc, u"auc curve: mean auc:%s"%mean_auc], loc='upper left', handlelength=2, handletextpad=0.5, borderpad=0.1)
    plt.ylim((0.3, 0.8))
    plt.show()

plot_accuracy_curve(os.path.join(raw_data_dir, "factor_linear.csv"))


# ### 3.3ã€å› å­æµ‹è¯• 
# * å¯¹å› å­è¿›è¡Œäº”åˆ†ä½åˆ†ç»„ï¼Œå¹¶è¿›è¡Œå›æµ‹
# * è€—æ—¶8åˆ†é’Ÿå·¦å³

# In[ ]:

import pandas as pd
import numpy as np
signal_df = pd.read_csv("raw_data/factor_linear.csv", dtype={"ticker": np.str, "tradeDate": np.str},index_col=0, encoding='GBK')
signal_df['ticker'] = signal_df['ticker'].apply(lambda x: str(x).zfill(6))
signal_df['ticker'] = signal_df['ticker'].apply(lambda x: x+'.XSHG' if x[:2] in ['60'] else x+'.XSHE')
signal_df = signal_df[[u'ticker', u'tradeDate', u'factor']]

signal_df.head()


# In[ ]:

import time
from CAL.PyCAL import * 

start_time = time.time()
# -----------å›æµ‹å‚æ•°éƒ¨åˆ†å¼€å§‹ï¼Œå¯ç¼–è¾‘------------
start = '2011-01-01'                       # å›æµ‹èµ·å§‹æ—¶é—´
end = '2017-12-31'                         # å›æµ‹ç»“æŸæ—¶é—´
benchmark = 'ZZ500'                        # ç­–ç•¥å‚è€ƒæ ‡å‡†
universe =set_universe('ZZ500')+set_universe('HS300')           # è¯åˆ¸æ± ï¼Œæ”¯æŒè‚¡ç¥¨å’ŒåŸºé‡‘
capital_base = 10000000                     # èµ·å§‹èµ„é‡‘
freq = 'd'                              
refresh_rate = Monthly(1)  

factor_data = signal_df[['ticker', 'tradeDate', 'factor']]     # è¯»å–å› å­æ•°æ®
factor_data = factor_data.set_index('tradeDate', drop=True)
q_dates = factor_data.index.values

accounts = {
    'fantasy_account': AccountConfig(account_type='security', capital_base=10000000)
}

# ---------------å›æµ‹å‚æ•°éƒ¨åˆ†ç»“æŸ----------------

# æŠŠå›æµ‹å‚æ•°å°è£…åˆ° SimulationParameters ä¸­ï¼Œä¾› quick_backtest ä½¿ç”¨
sim_params = quartz.SimulationParameters(start, end, benchmark, universe, capital_base, refresh_rate=refresh_rate, accounts=accounts)
# è·å–å›æµ‹è¡Œæƒ…æ•°æ®
data = quartz.get_backtest_data(sim_params)
# è¿è¡Œç»“æœ
results = {}

# è°ƒæ•´å‚æ•°(é€‰å–è‚¡ç¥¨çš„é›†æˆå› å­äº”åˆ†ä½æ•°)ï¼Œè¿›è¡Œå¿«é€Ÿå›æµ‹
for quantile_five in range(1, 6):
    
    # ---------------ç­–ç•¥é€»è¾‘éƒ¨åˆ†----------------
    
    def initialize(context):                   # åˆå§‹åŒ–è™šæ‹Ÿè´¦æˆ·çŠ¶æ€
        pass

    def handle_data(context): 
        account = context.get_account('fantasy_account')
        current_universe = context.get_universe('stock', exclude_halt=True)
        pre_date = context.previous_date.strftime("%Y%m%d")
        if pre_date not in q_dates:            
            return

        # æ‹¿å–è°ƒä»“æ—¥å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„å› å­ï¼Œå¹¶æŒ‰ç…§ç›¸åº”ååˆ†ä½é€‰æ‹©è‚¡ç¥¨
        q = factor_data.ix[pre_date].dropna()
        q = q.set_index('ticker', drop=True)
        q = q.ix[current_universe]
        
        q_min = q['factor'].quantile((quantile_five-1)*0.2)
        q_max = q['factor'].quantile(quantile_five*0.2)
        my_univ = q[(q['factor']>=q_min) & (q['factor']<q_max)].index.values

       # äº¤æ˜“éƒ¨åˆ†
        positions = account.get_positions()
        sell_list = [stk for stk in positions if stk not in my_univ]
        for stk in sell_list:
            account.order_to(stk,0)
        
        # åœ¨ç›®æ ‡è‚¡ç¥¨æ± ä¸­çš„ï¼Œç­‰æƒä¹°å…¥
        for stk in my_univ:
            account.order_pct_to(stk, 1.0/len(my_univ))


    # ç”Ÿæˆç­–ç•¥å¯¹è±¡
    strategy = quartz.TradingStrategy(initialize, handle_data)
    # ---------------ç­–ç•¥å®šä¹‰ç»“æŸ----------------
    
    # å¼€å§‹å›æµ‹
    bt, perf = quartz.quick_backtest(sim_params, strategy, data=data)

    # ä¿å­˜è¿è¡Œç»“æœï¼Œ1ä¸ºå› å­æœ€å¼ºç»„ï¼Œ5ä¸ºå› å­æœ€å¼±ç»„
    results[6 - quantile_five] = {'max_drawdown': perf['max_drawdown'], 'sharpe': perf['sharpe'], 'alpha': perf['alpha'], 'beta': perf['beta'], 'information_ratio': perf['information_ratio'], 'annualized_return': perf['annualized_return'], 'bt': bt}    

    print ('backtesting for group %s..................................' % str(quantile_five)),
print ('Done! Time Cost: %s seconds' % (time.time()-start_time))


# In[ ]:

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_style('white')

fig = plt.figure(figsize=(10,8))
fig.set_tight_layout(True)
ax1 = fig.add_subplot(211)
ax2 = fig.add_subplot(212)
ax1.grid()
ax2.grid()

for qt in results:
    bt = results[qt]['bt']

    data = bt[[u'tradeDate',u'portfolio_value',u'benchmark_return']]
    data['portfolio_return'] = data.portfolio_value/data.portfolio_value.shift(1) - 1.0   # æ€»å¤´å¯¸æ¯æ—¥å›æŠ¥ç‡
    data['portfolio_return'].ix[0] = data['portfolio_value'].ix[0]/	10000000.0 - 1.0
    data['excess_return'] = data.portfolio_return - data.benchmark_return                 # æ€»å¤´å¯¸æ¯æ—¥è¶…é¢å›æŠ¥ç‡
    data['excess'] = data.excess_return + 1.0
    data['excess'] = data.excess.cumprod()                # æ€»å¤´å¯¸å¯¹å†²æŒ‡æ•°åçš„å‡€å€¼åºåˆ—
    data['portfolio'] = data.portfolio_return + 1.0     
    data['portfolio'] = data.portfolio.cumprod()          # æ€»å¤´å¯¸ä¸å¯¹å†²æ—¶çš„å‡€å€¼åºåˆ—
    data['benchmark'] = data.benchmark_return + 1.0
    data['benchmark'] = data.benchmark.cumprod()          # benchmarkçš„å‡€å€¼åºåˆ—
    results[qt]['hedged_max_drawdown'] = max([1 - v/max(1, max(data['excess'][:i+1])) for i,v in enumerate(data['excess'])])  # å¯¹å†²åå‡€å€¼æœ€å¤§å›æ’¤
    results[qt]['hedged_volatility'] = np.std(data['excess_return'])*np.sqrt(252)
    results[qt]['hedged_annualized_return'] = (data['excess'].values[-1])**(252.0/len(data['excess'])) - 1.0
    ax1.plot(data['tradeDate'], data[['portfolio']], label=str(qt))
    ax2.plot(data['tradeDate'], data[['excess']], label=str(qt))
    

ax1.legend(loc=0)
ax2.legend(loc=0)
ax1.set_ylabel(u"å‡€å€¼", fontproperties=font, fontsize=16)
ax2.set_ylabel(u"å¯¹å†²å‡€å€¼", fontproperties=font, fontsize=16)
ax1.set_title(u"å› å­ä¸åŒäº”åˆ†ä½æ•°åˆ†ç»„é€‰è‚¡å‡€å€¼èµ°åŠ¿", fontproperties=font, fontsize=16)
ax2.set_title(u"å› å­ä¸åŒäº”åˆ†ä½æ•°åˆ†ç»„é€‰è‚¡å¯¹å†²ä¸­è¯500æŒ‡æ•°åå‡€å€¼èµ°åŠ¿", fontproperties=font, fontsize=16)

# results è½¬æ¢ä¸º DataFrame
results_pd = pd.DataFrame(results).T.sort_index()

results_pd = results_pd[[u'alpha', u'beta', u'information_ratio', u'sharpe', u'annualized_return', u'max_drawdown',  
                         u'hedged_annualized_return', u'hedged_max_drawdown', u'hedged_volatility']]

cols = [(u'é£é™©æŒ‡æ ‡', u'Alpha'), (u'é£é™©æŒ‡æ ‡', u'Beta'), (u'é£é™©æŒ‡æ ‡', u'ä¿¡æ¯æ¯”ç‡'), (u'é£é™©æŒ‡æ ‡', u'å¤æ™®æ¯”ç‡'), (u'çº¯è‚¡ç¥¨å¤šå¤´æ—¶', u'å¹´åŒ–æ”¶ç›Š'),
        (u'çº¯è‚¡ç¥¨å¤šå¤´æ—¶', u'æœ€å¤§å›æ’¤'), (u'å¯¹å†²å', u'å¹´åŒ–æ”¶ç›Š'), (u'å¯¹å†²å', u'æœ€å¤§å›æ’¤'), (u'å¯¹å†²å', u'æ”¶ç›Šæ³¢åŠ¨ç‡')]
results_pd.columns = pd.MultiIndex.from_tuples(cols)
results_pd.index.name = u'äº”åˆ†ä½ç»„åˆ«'
results_pd


# In[ ]:



